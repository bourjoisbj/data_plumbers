
##### IMPORT DATA FROM SQL USING SQOOP

### INSTALL SQOOP

sudo apt-get update
cd Desktop/opt
wget http://archive.apache.org/dist/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz

rm sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
ls
mv sqoop-1.4.7.bin__hadoop-2.6.0/ sqoop-1.4.7
cd sqoop-1.4.7/

## CONFIGURE SQOOP

# Get the file's path 
pwd
cd conf
ls
cp sqoop-env-template.sh sqoop-env.sh

sudo gedit sqoop-env.sh

## Uncomment these 2 following export in sqoop-env.sh and paste the path previously gotten

#Set path to where bin/hadoop is available
export HADOOP_COMMON_HOME=/home/fiedemploye/opt/hadoop-3.1.3

#Set path to where hadoop-*-core.jar is available
 export HADOOP_MAPRED_HOME=/home/fiedemploye/opt/hadoop-3.1.3

# Update the bash_profile file
cd
sudo gedit .bash_profile
 
## SQOOP HOME
export SQOOP_HOME=/home/fiedemploye/opt/sqoop-1.4.7
export PATH=$PATH:$SQOOP_HOME/bin

source .bash_profile
sqoop --version



### CREATE USER FOR REMOTE ACCESS OF DATABASE #################################

## NEW USER MYSQL ACCESS REMOTELY
https://medium.com/@nitingupta.bciit/setup-sqoop-to-import-mysql-data-to-hdfs-on-ubuntu-16-04-5243d9eef560

# Login as root to mysql
mysql -u root -p

# Create new mySql login user named as “sqoop_test”
mysql> CREATE USER ‘sqoop_test’@’localhost’ IDENTIFIED BY ‘password’;

# Provide new user “sqoop_test” privilege to connect to “hadoop_test” db.
mysql> GRANT ALL PRIVILEGES ON hadoop_test.* TO ‘sqoop_test’@’localhost’;
mysql> FLUSH PRIVILEGES;


## Verify new DB/Table/User are working in mySql
mysql -u sqoop_test -p


## Access locally the database
    $ sudo mysql

## Create a database for hadoop
    $ create DATABASE hadoop_test;
    $ use hadoop_test;

## Create a new table user
    $ Create table user(name varchar(20));

## Create new mySql login user named as "sqoop_user"
    $ CREATE USER 'sqoop_user'@'localhost' IDENTIFIED BY 'Password';

## Provide new user "sqoop_user" privilege to connect to "hadoop_test" database.
    $ GRANT ALL PRIVILEGES ON hadoop_test.* TO 'sqoop_user'@'localhost';
    $ FLUSH PRIVILEGES;

## Login to mysql using new user "sqoop_user" 
    $ mysql -u sqoop_user -p



### ACCESSING SQOOP FOR IMPORT ################################################
test sqoop 

sqoop import --connect jdbc:mysql://localhost/hadoop_test --username sqoop_user --password Password --table employee --target-dir /home/fiedemploye/opt/hadoop-3.1.3/hdfs --m 2

## We need to download mysql connector for Sqoop to work properly

wget  https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.49.tar.gz
tar zxvf mysql-connector-java-5.1.49.tar.gz
rm mysql-connector-java-5.1.49.tar.gz
ls

sudo cp /home/fieldemploye/opt/mysql-connector-java-5.1.49-bin.jar /home/fieldemploye/opt/sqoop-1.4.7/lib



### IMPORT USING THE COMMAND BELOW #############################################

sqoop import --connect jdbc:mysql://localhost/hadoop_test --username sqoop_user --password Password --table employee --target-dir /home/fiedemploye/opt/hadoop-3.1.3/hdfs --m 2 


pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
cf0 ## SQOOP COMMAND TO KNOW
Import \
    sqoop import --connect jdbc:mysql://localhost/db --username [mysql username] -p( password) -m [numb of mappers] --table [table name] --target-dir [hdfs directory]

Export
    sqoop export --connect jdbc:mysql://localhost/db --username [mysql username] -p( password) -m [numb of mappers] --table [table name] --export-dir [source]

Sqoop Job
    sqoop job --create myjob --[type] --connect jdbc:mysql://localhost/db --username [mysql username] -p(if password needed) --table [table name]

