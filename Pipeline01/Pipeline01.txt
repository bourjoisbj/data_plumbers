##################### Pipeline 01 ###############################

Twitter - Flume - HDFS - Spark - MySQL


# Set Flume configuration
cd opt/flume-1.8.0/conf
ls
sudo gedit flume-conf.properties

# Edit the file with the following configurations and save
#### Flume Configuration

# Naming the components on the current agent. 
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS

# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey = 
TwitterAgent.sources.Twitter.consumerSecret = 
TwitterAgent.sources.Twitter.accessToken = 
TwitterAgent.sources.Twitter.accessTokenSecret =
TwitterAgent.sources.Twitter.keywords = chicago
TwitterAgent.sources.Twitter.language = en
TwitterAgent.sources.Twitter.count = 1000

# Describing/Configuring the sink 
TwitterAgent.sinks.HDFS.type = hdfs 
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/twitFlumSprk/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream 
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.fileSuffix = .json 
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0 
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000 

# Describing/Configuring the channel 
TwitterAgent.channels.MemChannel.type = memory 
TwitterAgent.channels.MemChannel.capacity = 10000 
TwitterAgent.channels.MemChannel.transactionCapacity = 100

# Binding the source and sink to the channel 
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channel = MemChannel  

### Twitter to Flume to HDFS
# Start Twitter Ingestion with Flume
flume-ng agent -n TwitterAgent -c conf -f ~/opt/flume-1.8.0/conf/flume-conf1.properties -Dflume.root.logger=DEBUG,console

# HDFS to Spark to Hive
# Write a pyspark to stream data from HDFS, transform it and send it to Hive
# The code looks like the following

# Python Code for Sending dataframes to Hive

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from collections import namedtuple
from pyspark.sql import SparkSession
import re 
import random
import sys
reload(sys)
sys.setdefaultencoding('utf-8')

spark = SparkSession.builder.appName("WordCountSpark").getOrCreate()

sc = SparkContext.getOrCreate()
ssc = StreamingContext(sc, 25)

fields = ("id","hashtag", "count", "length" )
Tweet = namedtuple( 'Tweet', fields )

def toSQL(dataframes):
	dataframes.show()
	dataframes.write.format("jdbc")\
	.mode("overwrite")\
	.option("url", "jdbc:mysql://localhost:3306/TweetTable") \
	.option("dbtable", "ht") \
	.option("user", "sql user") \
	.option("password", "Welcome2BB@") \
	.option("driver", "com.mysql.jdbc.Driver") \
	.save()


def savetheresult( rdd ):
    if not rdd.isEmpty():
    	dataframes = spark.createDataFrame(rdd)
    	toSQL(dataframes)
 
lines = ssc.textFileStream("hdfs://localhost:9000//TwitFlumSprk")


counts = lines.flatMap( lambda text: text.split( " " ))\
.filter( lambda word: word.lower().startswith("#") )\
.map(lambda word: word.replace('#',''))\
.map(lambda word: word.lower())\
.filter(lambda word: re.sub('[^a-z]+', '', word))\
.filter(lambda word: re.sub(r'[^\x00-\x7F]+',' ', word))\
.filter(lambda word: len(word)>1)\
.map( lambda word: ( word, 1 ) )\
.reduceByKey( lambda a, b: a + b )\
.map( lambda rec: Tweet( random.randint(1,100000) ,rec[0], rec[1], len(rec[0]) ) )\
.foreachRDD(lambda x: savetheresult(x))

ssc.start()
ssc.awaitTermination()


# Run Spark submit with a jar corresponding jar file.
Spark-submit 

# Launch mySql remotely 
mysql -u 'sql user' -p

#
Create database TweetTable;

# Create table according to the table in the spark codes with the same fields
